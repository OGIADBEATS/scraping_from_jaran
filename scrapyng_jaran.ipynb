from google.colab import drive
drive.mount('./gdrive')

!pip install selenium

from bs4 import BeautifulSoup
import requests
import urllib.robotparser
import pandas as pd
from selenium import webdriver
from selenium.webdriver.common.by import By
import time

############################################

#-----スクレイピング-----#

ranking_url = "https://www.jalan.net/jalan/doc/ranking/total/03_index.html"

#スクレイピング可能か調べる
robots_url = "https://www.jalan.net/robots.txt"

rp = urllib.robotparser.RobotFileParser()
rp.set_url(robots_url)
rp.read()

user_agent = '*'
result = rp.can_fetch(user_agent, ranking_url)
print(result)

#-----クローリング-----#

#ブラウザをheadlessモード実行
options = webdriver.ChromeOptions()
#ヘッドレスモード（バックグラウンドで起動）で実行。コラボの場合、必須。
options.add_argument('--headless')
#サンドボックスモードの解除。これも必須。
options.add_argument('--no-sandbox')
#これも設定した方がよい。
options.add_argument('--disable-dev-shm-usage')
#おまじない
driver = webdriver.Chrome(options=options)
driver.implicitly_wait(10)

ranking_url = "https://www.jalan.net/jalan/doc/ranking/total/03_index.html"
base_url = "https://www.jalan.net"
#ランキング画面のスクレイピング
driver.get(ranking_url)
driver.implicitly_wait(10)
content_element = driver.find_element(By.CLASS_NAME, "content")
content_html = content_element.get_attribute('outerHTML')

#旅館ランキングのhtml抽出
def ryokan():
  element = driver.find_element(By.ID, "total1")
  ranking_html = element.get_attribute('outerHTML')
  #BeautifulSoup オブジェクトに変換
  ranking_soup = BeautifulSoup(ranking_html, 'html.parser')
  return ranking_soup

#ランキングの宿名, URLを取得
def ranking():
  ranking_soup = ryokan()
  rankings = ranking_soup.find_all("div", class_="item")

  for ranking in rankings:
    name = ranking.find("h4", class_="title").text.strip()
    tag_a = ranking.select_one("a")
    href_link = tag_a.get("href")
    links = base_url + href_link
    print(name)
    print(links)

ranking()
